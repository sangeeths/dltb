#!/bin/bash

usage() {
    echo
	echo "Usage:"
    echo
    echo "dltb [-c|--crawl]
     [-d|--debug] 
     [-l|--log <filename>]
     [-u|--url <url>]"
    echo
    echo "-c|--crawl        After downloading mp3's, explore this
                  webpage and crawl the hyperlinks to 
                  find more mp3's; Default: off"
    echo
    echo "-d|--debug        Turn debug on; Default: off"
    echo
    echo "-l|--log <file>   Enable logging to <file>; Default: off"
    echo
    echo "-u|--url          Tamilbeat url;
                  Default: $default_url"
    echo
	exit 
}

##
# Single line and double line used for printing
# only in debug modes
#
single_line=$(seq -s "-" 60 | sed 's/[0-9]//g')
double_line=$(seq -s "=" 60 | sed 's/[0-9]//g')

##
# Parse the given url and gets the movie name
# and stores it in $mn
#
get_mn() {
	local a=(`echo $1 | tr "/" " "`)
	local len=${#a[@]}
	mn=`echo ${a[len-2]} | sed 's/%20/_/g'` 
	mkdir -p $mn
}

##
# Parse the given url and get the song name
# and stores it in $sn
#
get_sn() {
	local a=(`echo $1 | tr "/" " "`)
	local len=${#a[@]}
	sn=`echo ${a[len-1]} | sed 's/TamilBeat.Com%20-%20//g' | sed 's/%20/_/g'` 
}

# Browse the page and see whether there is any
# mp3 url available for download
#
# If mp3 not present:
#   returns
#
# If mp3 present: 
#   surls:  This is an array of all the mp3 urls
#   mn:     This contains the movie name 
#   sn:     This contains the song name 
#
# For all the urls in the $surls,
#   0. Ignore any blank lines (glitch)
#   1. get the song name
#   2. form the wget command
#   3. execute the wget command
#
dload_mp3() {
    # get the url content
	GET $1 > $tb_f
    
    # If the crawled url doesn't contain mp3's 
    # then return
	[[ $(cat $tb_f | grep "\.mp3" | wc -l) -eq 0 ]] && return 

    # All the available mp3 url's will be stored
    # in $surls 
	local surls=(`cat $tb_f | grep "\.mp3" | awk 'BEGIN{FS="href=\""}{print $2}' | sed 's/\">/\n/g'`)
	
    # The movie name will be available in $mn
    get_mn ${surls[0]}

	for surl in ${surls[@]}
	do
	    # Ignore blank lines, if any!
    	[[ $(echo $surl | grep "\.mp3" | wc -l) -eq 0 ]] && continue
        
        # Get the song name from the url
		get_sn $surl

        # Build the command and execute it!
    	echo "D. $mn/$sn"
	    command="wget -q -O $mn/$sn $surl"
    	$command
	done
}

##
# Displays the master list, which contains all 
# URLs the is being worked on.
#
# This function will be called mostly on 
# debug mode
#
display_master_list() {
    echo $double_line
    echo "       List of url's in the master-list"
    echo $single_line
    for tb_url in ${tb_urls[@]}
    do
        echo $tb_url
    done	
    echo $double_line
}

## 
# Check whether the incoming url is present
# in the master list ($tb_urls).
# 
# If present, ignore.
# If not present, then add it to the master list
#
add_url_to_crawl_list() {
	for u in ${tb_urls[*]}
	do
		if [[ $u == $1 ]] ; then
			[[ $debug -eq 1 ]] && echo "Ignoring - $u"
			return
		fi
	done
		
	[[ $debug -eq 1 ]] && echo "Adding - $1"
    
    # add the incoming url to the master list
	tb_urls=("${tb_urls[@]}" $1)
	return
}

##
# Paser the incoming url to find more url's to crawl
#
update_tb_urls() {
	echo "C. $1"
	GET $1 > $tb_f
    
    # $base_url is just the domain name
    # f.e. http://www.tamilbeat.com
    # http://www.tamilat.com
	local base_url=`echo $1 | sed 's/\.com.*/\.com/g'`

    # $onel_url is the domain name + the immediate folder
    # f.e http://www.tamilbeat.com/tamilsongs
    #
    # NOTE: 
    # $base_url and $onel_url are needed to parse and crawl 
    # the url's which starts with / or ../
    # f.e. if we come across a href="../abc" then it means
    # $onel_url + abc
	if [[ $1 =~ \.*.html$ ]] ; then
    	local onel_url=`echo $1 | sed 's/\/[a-zA-Z0-9%_.-]*\/[a-zA-Z0-9%_.-]*$//g'`
	else
	    local onel_url=`echo $1 | sed 's/\/[a-zA-Z0-9%_.-]*$//g'`
	fi

	local i=0

    # The given url's are parsed for new unique url's. 
    # They are stored in $crawl_wp_urls array
	local crawl_wp_urls=(`cat $tb_f | awk 'BEGIN{FS="href=\""}{print $2}' | sed '/^$/d' | sed 's/\".*//g' | sed 's/\/$//g' | uniq`)

	for url in ${crawl_wp_urls[*]}
	do
	    if [[ $url =~ ^\/ ]] ; then
    	    crawl_wp_urls[$i]="$base_url$url"
	    fi 

	    if [[ $url =~ ^\.\. ]] ; then
    	    local temp=`echo $url | sed 's/^\.\.//g'`
        	crawl_wp_urls[$i]="$onel_url$temp"
	    fi  
		
		if [[ ${crawl_wp_urls[$i]} =~ ^http ]] ; then 
			add_url_to_crawl_list ${crawl_wp_urls[$i]}
		fi

		let i=$i+1
	done

	return
}

##
# $tb_urls = master list of all the url's 
# $index = the url that is currently being 
#          worked on..
#
# if $index < size_of(tb_urls) ; then
#   1. Take the index_url (then increment $index)
#       1.1. Find whether there is any mp3 files
#           1.1.1. If any, download them
#   2. If crawl is enabled (-c | --crawl), then 
#      crawl this page, parse the urls and 
#      update the master list (tb_urls)
#
tb_crawl() {
    [[ $debug -eq 1 ]] && display_master_list

    # If there are more url's to be crawled
	if [[ $index -lt ${#tb_urls[*]} ]] ; then 

		index_url=${tb_urls[$index]}
		let index=$index+1

        dload_mp3 $index_url

        # if -c | --crawl option is set, then
        # crawl the next url in the master list 
        if [[ $crawl -eq 1 ]] ; then 
			update_tb_urls $index_url
            tb_crawl
        fi

	fi
}

##
# default URL
default_url="http://www.tamilbeat.com/tamilsongs"

##
# Index of the master list - tb_urls
index=0

##
# temp file which stores the webpage
# content; used for all string parsing
tb_f=`pwd`/.tb_f

##
# Global variables
# initialize it to default values
tb_urls=$default_url
debug=0
log=0
crawl=0

##
# Process command line arguments and
# set all the global arguments
while [[ "$1" != "" ]] ; do
    case $1 in
        --debug | -d )      debug=1
                            ;;  
        -u | --url )        shift
                            url=1
                            tb_urls=$1  
                            ;;  
        -l | --log )        shift
                            log=1
                            log_file=$1
                            ;;  
        -c | --crawl )      crawl=1
                            ;;  
        * )                 usage
                            exit
        esac
        shift
done

##
# The following prints the command line
# values on the debug mode
if [[ $debug -eq 1 ]] ; then    
    echo "The command line arugments are :"
    if [[ $debug -eq 1 ]] ; then
        echo "debug = on"
    else
        echo "debug = off"
    fi

    echo "url = $tb_urls"

    if [[ $log -eq 1 ]] ; then
        echo "log = $log_file"
    else
        echo "log = off"
    fi
    
    if [[ $crawl -eq 1 ]] ; then
        echo "crawl = on"
    else
        echo "crawl = off"
    fi
fi

tb_crawl 

