#!/bin/bash

##
#
# Author: Sangeeth Saravanaraj
# E-mail: sangeeth.saravanaraj@gmail.com
#
##

##
# out function prints to the stdout
#
# $1 could be one of the following
#   1. d = debug
#   2. p = print
#   3. l = log
#
# $2 contains the content to the printed!
#
out() {
    local stdout=0
    if [[ $1 =~ .*d.* ]] && [[ $debug -eq 1 ]] ; then 
        echo $2
        stdout=1
    fi  
    [[ $1 =~ .*p.* ]] && [[ $stdout -eq 0 ]] && echo $2
    [[ $1 =~ .*l.* ]] && [[ $log -eq 1 ]] && echo $2 >> $log_file
}

##
# displays help!
#
usage() {
    echo 
	echo "Usage:"
    echo 
    echo "dltb [-c|--crawl]"
    echo "     [-d|--debug]"
    echo "     [-l|--log <filename>]"
    echo "     [-u|--url <url>]"
    echo 
    echo "-c|--crawl        After downloading mp3's, explore this"
    echo "                  webpage and crawl the hyperlinks to"
    echo "                  find more mp3's; Default: off"
    echo 
    echo "-d|--debug        Turn debug on; Default: off"
    echo 
    echo "-l|--log <file>   Enable logging to <file>; Default: off"
    echo 
    echo "-u|--url          Tamilbeat url;"
    echo "                  Default: $default_url"
    echo 
	exit 
}

##
# Single line and double line used for printing
# only in debug modes
#
single_line=$(seq -s "-" 60 | sed 's/[0-9]//g')
double_line=$(seq -s "=" 60 | sed 's/[0-9]//g')

##
# Parse the given url and gets the movie name
# and stores it in $mn
#
get_mn() {
	local a=(`echo $1 | tr "/" " "`)
	local len=${#a[@]}
	mn=`echo ${a[len-2]} | sed 's/%20/_/g'` 
	mkdir -p $mn
}

##
# Parse the given url and get the song name
# and stores it in $sn
#
get_sn() {
	local a=(`echo $1 | tr "/" " "`)
	local len=${#a[@]}
	sn=`echo ${a[len-1]} | sed 's/TamilBeat.Com%20-%20//g' | sed 's/%20/_/g'` 
}

# Browse the page and see whether there is any
# mp3 url available for download
#
# If mp3 not present:
#   returns
#
# If mp3 present: 
#   surls:  This is an array of all the mp3 urls
#   mn:     This contains the movie name 
#   sn:     This contains the song name 
#
# For all the urls in the $surls,
#   0. Ignore any blank lines (glitch)
#   1. get the song name
#   2. form the wget command
#   3. execute the wget command
#
dload_mp3() {
    # get the url content
	GET $1 > $tb_f
    
    # If the crawled url doesn't contain mp3's 
    # then return
	[[ $(cat $tb_f | grep "\.mp3" | wc -l) -eq 0 ]] && return 

    # All the available mp3 url's will be stored
    # in $surls 
	local surls=(`cat $tb_f | grep "\.mp3" | awk 'BEGIN{FS="href=\""}{print $2}' | sed 's/\">/\n/g'`)
	
    # The movie name will be available in $mn
    get_mn ${surls[0]}

	for surl in ${surls[@]}
	do
	    # Ignore blank lines, if any!
    	[[ $(echo $surl | grep "\.mp3" | wc -l) -eq 0 ]] && continue
        
        # Get the song name from the url
		get_sn $surl

        # Build the command and execute it!
    	out "ldp" "D. $mn/$sn"
	    command="wget -q -O $mn/$sn $surl"
    	$command
	done
}

##
# Displays the master list, which contains all 
# URLs the is being worked on.
#
# This function will be called mostly on 
# debug mode
#
display_master_list() {
    out "dl" "$double_line"
    out "dl" "       List of url's in the master-list"
    out "dl" "$single_line"
    for tb_url in ${tb_urls[@]}
    do
        out "dl" "$tb_url"
    done	
    out "dl" "$double_line"
}

## 
# Check whether the incoming url is present
# in the master list ($tb_urls).
# 
# If present, ignore.
# If not present, then add it to the master list
#
add_url_to_crawl_list() {
	for u in ${tb_urls[*]}
	do
		if [[ $u == $1 ]] ; then
			[[ $debug -eq 1 ]] && out "dl" "Ignoring - $u"
			return
		fi
	done
		
	[[ $debug -eq 1 ]] && out "dl" "Adding - $1"
    
    # add the incoming url to the master list
	tb_urls=("${tb_urls[@]}" $1)
}

##
# Paser the incoming url to find more url's to crawl
#
update_tb_urls() {
	out "dlp" "C. $1"
	GET $1 > $tb_f
    
    # $base_url is just the domain name
    # f.e. http://www.tamilbeat.com
    # http://www.tamilat.com
	local base_url=`echo $1 | sed 's/\.com.*/\.com/g'`

    # $onel_url is the domain name + the immediate folder
    # f.e http://www.tamilbeat.com/tamilsongs
    #
    # NOTE: 
    # $base_url and $onel_url are needed to parse and crawl 
    # the url's which starts with / or ../
    # f.e. if we come across a href="../abc" then it means
    # $onel_url + abc
	if [[ $1 =~ \.*.html$ ]] ; then
    	local onel_url=`echo $1 | sed 's/\/[a-zA-Z0-9%_.-]*\/[a-zA-Z0-9%_.-]*$//g'`
	else
	    local onel_url=`echo $1 | sed 's/\/[a-zA-Z0-9%_.-]*$//g'`
	fi

    # The given url's are parsed for new unique url's. 
    # They are stored in $crawl_wp_urls array
	local crawl_wp_urls=(`cat $tb_f | awk 'BEGIN{FS="href=\""}{print $2}' | sed '/^$/d' | sed 's/\".*//g' | sed 's/\/$//g' | uniq`)

	for url in ${crawl_wp_urls[*]}
	do
        # The following are the _black_listed_items_
        # so, ignore them!
        [[ $url =~ \.mp3$ ]] && continue
        [[ $url =~ \.m3u$ ]] && continue
        [[ $url =~ \.css$ ]] && continue
        [[ $url =~ .*\/cinenews\/.* ]] && continue
        [[ $url =~ .*\/gallery\/.* ]] && continue
        [[ $url =~ .*\/trailers\/.* ]] && continue

        # Any url that starts with a / should be
        # resolved with the $base_url
	    if [[ $url =~ ^\/ ]] ; then
    	    url="$base_url$url"
	    fi 

        # Any url that starts with .. should be 
        # resolved with the #onel_url
	    if [[ $url =~ ^\.\. ]] ; then
    	    local temp=`echo $url | sed 's/^\.\.//g'`
        	url="$onel_url$temp"
	    fi  
            
        # After all the above validations, the url
        # should start with http to be considered to be 
        # a valid url. If found valid, then add 
        # to master list
		if [[ $url =~ ^http\:\/\/www\. ]] ; then 
			add_url_to_crawl_list $url
		fi
	done
}

##
# $tb_urls = master list of all the url's 
# $index = the url that is currently being 
#          worked on..
#
# if $index < size_of(tb_urls) ; then
#   1. Take the index_url (then increment $index)
#       1.1. Find whether there is any mp3 files
#           1.1.1. If any, download them
#   2. If crawl is enabled (-c | --crawl), then 
#      crawl this page, parse the urls and 
#      update the master list (tb_urls)
#
tb_crawl() {
    # if running in debug or log mode, then
    # display the master list
    display_master_list

    # If there are more url's to be crawled
	if [[ $index -lt ${#tb_urls[*]} ]] ; then 

		index_url=${tb_urls[$index]}
		let index=$index+1

        dload_mp3 $index_url

        # if -c | --crawl option is set, then
        # crawl the next url in the master list 
        if [[ $crawl -eq 1 ]] ; then 
			update_tb_urls $index_url
            tb_crawl
        fi
	fi
}

##
# default URL
default_url="http://www.tamilbeat.com/tamilsongs"

##
# default log file name
default_log_file="log_dltb"

##
# Index of the master list - tb_urls
index=0

##
# temp file which stores the webpage
# content; used for all string parsing
tb_f=`pwd`/.tb_f

##
# Global variables
# initialize it to default values
tb_urls=$default_url
log_file=$default_log_file
debug=0
log=0
crawl=0

##
# Process command line arguments and
# set all the global arguments
while [[ "$1" != "" ]] ; do
    case $1 in
        --debug | -d )      debug=1
                            ;;  
        -u | --url )        shift
                            url=1
                            tb_urls=$1  
                            ;;  
        -l | --log )        shift
                            log=1
                            log_file=$1
                            ;;  
        -c | --crawl )      crawl=1
                            ;;  
        * )                 usage
                            exit
        esac
        shift
done

##
# The following prints the command line
# values on the debug mode
out "dl" "The command line arugments are :"
out "dl" "url       = $tb_urls"
[[ $debug -eq 1 ]] && out "dl" "debug   = on"
[[ $debug -ne 1 ]] && out "dl" "debug   = off"
[[ $log -eq 1 ]]   && out "dl" "log     = $log_file"
[[ $log -ne 1 ]]   && out "dl" "log     = off"
[[ $crawl -eq 1 ]] && out "dl" "crawl   = on"
[[ $crawl -ne 1 ]] && out "dl" "crawl   = off"

tb_crawl 

# end #

